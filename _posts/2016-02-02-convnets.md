---
layout: post
title: "Convolutional neural networks"
date: 2016-02-02
---

https://twitter.com/VisualGenome/status/682997407659982848


notes 
 - volumes should really be covered before this. this can review volumes
Convnets
 - intro, history
 - applications = links to other chapters
   - deepdream, styletransfer, go
 - weaknesses of neural networks
   - translation confusion
   - stretches, rotations, etc
 - convolution layer
   - convolution demo
   - convolution_all
 - pooling layers
 - review of volumes + the whole pipeline end to end
 - interpreting activations
 - hint of future chapters and applications


notes
 - have a section about feature representation. HOG/SIFT/handcrafted before + audio examples, replaced by learned features in deep learning


2010 (or 2009?) acero et al neural net for speech recognition swapped in for (gmm?) big boost on accuracy

2012 imagenet

alexnet = 15.4%
resnet '15 (152 layers) => ILSVRC 15 (he et al)
karen simonyan? vggnet
trends
 - removing fc + pooling layer for more convs
 - can remove pools by having convs w stride 2
 - smaller filters, more of them, and more layers

binary convnets

lenet
googlenet
alexnet
vggnet
resnet (microsoft)

batchnorm? 
localization - resnet 2015 dropped SOA from 25->9% localization error

---------

a common criticism of . one of the first concerted efforts to visualize convnets



We are now going to move on to a special type of neural network, called a convolutional neural network, CNN or convnet for short. Although CNNs have been around since ____, they have only recently achieved fame for improving the state-of-the-art in several fundamental tasks in machine learning. CNNs have arguably revolutionized [footnote] computer vision, a field which has historically been at the heart of much interactive and new media art. Starting in 2012, CNNs became the top-performing algorithms for the task of image classification, first doing so at the ILSVCR competition in which the SuperVision system designed by Krizhevsky and co achieved a 16% error on top-5 guesses, shattering the previous record by an unprecedented 10% margin. Since then, virtually all entries into the competition have been CNNs, and as of 2016, the current best sits at an astonishing 6%, [just 1% worse than a human](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)!

This dramatic improvement in our ability to design programs which accurately classify pictures has pushed the boundaries in a variety of higher-level applications, including:

---
cnns are the current top record holders in both image and speech classification. if you get through this you will fully understand at least in principle the current heavyweight champion algorithms known as of now in machine learning predictive  

cnns inherit everything from ordinary neural networks but build on top of them with several innovations which greatly improve its predictive accuracy as well as opening up a great many applications of cnns besides for ordinary regression and classification such as
---


 - recognizing objects, locations, and people in images
 - tracking the road and detecting obstacles in self-driving cars
 - automatically captioning or describing images and videos in natural language, as well as the reverse

CNNs were the workhorses behind Deepdream, style transfer, and many other visual art applications of machine learning that appeared in 2015. Furthermore, also sound processing __

What innovations do CNNs have that makes them so effective at image classification? To understand them, let's first address the weaknesses of regular neural networks.

# Translation = confusion

When we first looked at the example of handwritten digit classification using regular neural networks, we made the following observation. Different images of the same numeral showed considerable variation in where the pixels lit up. Consider the following sample of 4s. The variations in them force our hidden layers to learn weights somewhat uncertainly, creating a filter that looks like an average of all of them. Such a filter helps to take into account all the variations in handwritten 4s, but is problematic between the filter weights of different numerals begin to overlap a great deal, creating confusion between them.

4 4 4 4 4 4 4 -> learned filter

Let's consider a handwritten 1 and 7. In practice, a 1 overlaps with the vertical bar in a 7 quite frequently. Because the weights in the 7-neuron are diluted, the top-bar receives a weaker signal. So in many cases, our system will confuse a 7 for a 1 or vice-versa. Similarly it will confuse 8 and 9, 5 and 6, and so on. Humans find distinguishing most people's 7s from their 1s easy - they simply observe that the top bar isn't there. But in regular neural networks, we don't have such a variable as \"top bars,\" we simply have the individual pixels. 


Perhaps, we would be able to build a better classifier if we _were_ able to represent objects encoded by multiple pixels, such as the \"top bars\" in 7s. We might characterize the 6 as a curved vertical arch and a loop at the bottom, and that a __ is a __.  Moreover, we could also say a cat is two eyes, a nose, some whiskers and ears, and so on. But let's not get ahead of ourselves yet.

# Learning higher-level features

CNNs offer us an an approach built on top of regular neural networks, which follows from the supposition made in the last paragraph. When they were first applied to handwritten digit classification, the system produced by Yann LeCun and co in 1991 improved handwritten digit classification to 99.3%, leading it to be used for reading around 10% of the checks deposited at banks in the U.S. throughout the 90s. But CNNs _really_ thrive when we expand our task to attempt to classify not just handwritten digits, but thousands of other kinds of objects appearing in pictures, from airplanes to lobsters to iPods and everything in between.

CNNs achieve this by 

# Why hard

stretches, rotations, scalings, variants of things (chair)



# Convolutional layers


# Pooling layers

Recently, some research groups have reported promising results with CNNs without pooling layers so these might be a thing of the past soon. See how quickly things move!!


## Example of digit

Take a look at the following two images of the number zero. 
They 



#### Limitations of regular neural networks

translation invariance


etc
lecun 89 -> first CNN according to zeiler (schmidhuber?)




notes
 - 3.08% top-5 on imagenet with residual connections + inception architecture http://arxiv.org/abs/1602.07261


convolution visual cortex responds to local receptive fields
convolution animation inspired from dl book show conv filter sliding across image generating dots

etc
 - http://karpathy.github.io/2015/10/25/selfie/
 - @fulhack typography