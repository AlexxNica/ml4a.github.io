---
layout: post
title: "Introduction"
date: 2016-01-02
---


\"Artificial intelligence is anything computers [can\'t do yet](https://en.wikipedia.org/wiki/AI_effect)\" - Douglas Hofstadter, re: Tesler theorem 

\"Machine learning is just applied computational statistics.\" - Chris Wiggins

Evidently, the meaning of the terms _artificial intelligence_ and _machine learning_ depend on who you ask.
	
The term artificial intelligence has been in [usage since the 1950s](https://en.wikipedia.org/wiki/History_of_artificial_intelligence), and originally referred to ___. It has been prone to periods of great popular interest, and [pessimism and decreased interest](__ai winter__), in part due to its failure to achieve the sort of techno-utopianism prognosticated by [crackpot speculators](https://en.wikipedia.org/wiki/The_Singularity_Is_Near).

Pamela McGoldrick attributes these periodic busts to the notion that \"Practical AI successes, computational programs that actually achieved intelligent behavior, were soon assimilated into whatever application domain they were found to be useful in, and became silent partners alongside other problem-solving approaches, which left AI researchers to deal only with the \'failures.\'\" 

Similarly, common usage of the term \"machine learning\" goes back to the 1980s, but most of its core concepts trace back much further than the term itself. It became widely recognized as a field in its own right when approaches from statistics - especially data-driven computational statistics - began being applied to problems in AI.

ML has become so predominant in AI that the terms are often used interchangeably, both inside and outside of academia.

## Emergence of deep learning

In the 2010s, many machine learning researchers who were working with multilayer (\"deep\") architectures - especially multilayer neural networks - began to use the term \"deep learning.\" 

Deep neural networks set new benchmarks in speech recognition in 2009 (citation) and image recognition in 2012 (citation) - shattering the previous year\'s top systems by an unprecedented margin - and have since characterized most of ML\'s state-of-the-art techniques. In the last 5 years, most of the major scientific research centers have increased their funding of ML research, particularly deep learning. Much of the research has shifted away from university research centers and to the major tech industry titans, including Facebook, Google, Microsoft, and Amazon. Owing to its notorious secrecy, [Apple has largely fallen behind](http://www.bloomberg.com/news/articles/2015-10-29/apple-s-secrecy-hurts-its-ai-software-development).

In 2015 [JÃ¼rgen Schmidhuber](http://people.idsia.ch/~juergen/) wrote [\"A
Critique of Paper by \'Deep Learning Conspiracy\'\"](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html), recasting the recent advances of deep learning as building on top of prior work with multilayer neural networks, going back decades. His post provoked a great deal of commentary from numerous people, including very well-known researchers, sparking arguably [the most complicated internet flamewar of all-time](https://plus.google.com/100849856540000067209/posts/9BDtGwCDL7D).

## Deep learning shmeep learning

The point is that none of these terms have well-defined meanings, and connotations are inconsistently associated with them, especially within the academic community itself. We are interested in machines which _do_ interesting things, and can do them _now_ or may do so in the near future.

## Core principles of this book
## Mathemagics

This book is not aimed to be scientific. plenty exists. nor is it. there is math, software, .. it is so you are capable of interrogating it. Machine learning is a large and fast-moving field so we can\'t get rid of everything. Instead, a computer science background is 


## Software 

This book tries to find a middle way. IT uses many different. such is the state of things. but tries to be methodical and easy to install, come with as many documented examples as possible.


## Upkeep

Machine learning is a fast-moving field and a quarter of the first draft of this book was obsolete by the time I finished writing it. I will try to keep up with the winds of change and welcome tips and contributions [github][twitter][email], and [your generous support](donate/paeon).


## Math

In general, this book will try to minimize the use of math, and rely on visual aides more than equations, both because neural networks can be well understood this way, and because it helps reduce the need for other qualifications. Nevertheless you may still find it helpful to review your maths to attach mental probabilistic and spatial geometric interpretations to them.
Francis Tseng has a nice guide to AI and ML which contains a concise review of the math needed.



one of the things we can do away with is some of the details of training deep networks and focus on applications. vindication for me as its all i used to do


contents
 - machine learning
 - neural nets
   - looking inside neural nets
   - how neural nets trained
 - convnets
   - looking inside convnets
   - encoding images (refer tSne)
   - deepdream
   - style transfer
 - ethics & issues
 ------
 - autoencoders 
   - @kyle audio, @dribnet
 - gans
   - @ABFTS, @manga, @flowers
 - rnn
   - @hardmaru, @fulhack, @karpathy, @kyle, @ageitgey mario
 - NLP
   - tf/idf -> t-sne

software
 - neuraltalk + walk
 - tSne + gridding (@olivia, @gene)
 - dimensionality reduction, PCA, self-organizing maps
 - tSne


pictures from arxiv pictures
 - http://cs231n.stanford.edu/slides/winter1516_lecture9.pdf