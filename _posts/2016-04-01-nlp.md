---
layout: post
title: "Language processing"
date: 2016-03-01
---


Quote

"the trophy doesn't fit in the suitcase because it is too big" - geoff hinton
"the trophy doesn't fit in the suitcase because it is too small" - geoff hinton

Trying to get algorithms to make sense of ambiguity of human language, we begin to appreciate just how much we take for granted. We hardly notice the tiny feats of disambiguation our brains do when reading sentences like the ones above.


This chapter is about applications of machine learning to natural language processing. like ml, NLP is a nebulous term with several precise definitions and most have something to do wth making sense from text. This chapter will take a broad view of NLP


“Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences.” -Dr. Christopher D. Manning, Dec 2015 

 - check manning pdf statement

quote/link from: http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/


word vectors

word vectors are a rep such that geometric preserved in emeddings.
reverse king queen

cover tf-idf in detail (link to it fromm  tsne chapter). link to t-sne chapter from here

since then, there have been a number of writings which have tried to interpret these word vectors. gender binary


tf-idf examples -> t-SNE examples

word2vec
 - analogies
 - kcimc antonyms
 - rejecting gender binary


tf-idf -> t-SNE
LSA + LDA -> t-SNE

RNNs annotating?


word2vec chapter
 - anything2vec http://www.lab41.org/anything2vec/




https://www.youtube.com/watch?v=XG-dwZMc7Ng
the trophy can't fit into the suitcase because it's too big (it = trophy)
the trophy can't fit into the suitcase because it's too small (it = suitcase)


